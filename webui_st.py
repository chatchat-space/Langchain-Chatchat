import streamlit as st
# from st_btn_select import st_btn_select
import tempfile
###### ä»webuiå€Ÿç”¨çš„ä»£ç  #####
######   åšäº†å°‘é‡ä¿®æ”¹    #####
import os
import shutil

from chains.local_doc_qa import LocalDocQA
from configs.model_config import *
import nltk
from models.base import (BaseAnswer,
                         AnswerResult,)
import models.shared as shared
from models.loader.args import parser
from models.loader import LoaderCheckPoint

nltk.data.path = [NLTK_DATA_PATH] + nltk.data.path


def get_vs_list():
    lst_default = ["æ–°å»ºçŸ¥è¯†åº“"]
    if not os.path.exists(KB_ROOT_PATH):
        return lst_default
    lst = os.listdir(KB_ROOT_PATH)
    if not lst:
        return lst_default
    lst.sort()
    return lst_default + lst


embedding_model_dict_list = list(embedding_model_dict.keys())
llm_model_dict_list = list(llm_model_dict.keys())
# flag_csv_logger = gr.CSVLogger()


def get_answer(query, vs_path, history, mode, score_threshold=VECTOR_SEARCH_SCORE_THRESHOLD,
               vector_search_top_k=VECTOR_SEARCH_TOP_K, chunk_conent: bool = True,
               chunk_size=CHUNK_SIZE, streaming: bool = STREAMING,):
    if mode == "Bingæœç´¢é—®ç­”":
        for resp, history in local_doc_qa.get_search_result_based_answer(
                query=query, chat_history=history, streaming=streaming):
            source = "\n\n"
            source += "".join(
                [f"""<details> <summary>å‡ºå¤„ [{i + 1}] <a href="{doc.metadata["source"]}" target="_blank">{doc.metadata["source"]}</a> </summary>\n"""
                 f"""{doc.page_content}\n"""
                 f"""</details>"""
                 for i, doc in
                 enumerate(resp["source_documents"])])
            history[-1][-1] += source
            yield history, ""
    elif mode == "çŸ¥è¯†åº“é—®ç­”" and vs_path is not None and os.path.exists(vs_path):
        for resp, history in local_doc_qa.get_knowledge_based_answer(
                query=query, vs_path=vs_path, chat_history=history, streaming=streaming):
            source = "\n\n"
            source += "".join(
                [f"""<details> <summary>å‡ºå¤„ [{i + 1}] {os.path.split(doc.metadata["source"])[-1]}</summary>\n"""
                 f"""{doc.page_content}\n"""
                 f"""</details>"""
                 for i, doc in
                 enumerate(resp["source_documents"])])
            history[-1][-1] += source
            yield history, ""
    elif mode == "çŸ¥è¯†åº“æµ‹è¯•":
        if os.path.exists(vs_path):
            resp, prompt = local_doc_qa.get_knowledge_based_conent_test(query=query, vs_path=vs_path,
                                                                        score_threshold=score_threshold,
                                                                        vector_search_top_k=vector_search_top_k,
                                                                        chunk_conent=chunk_conent,
                                                                        chunk_size=chunk_size)
            if not resp["source_documents"]:
                yield history + [[query,
                                  "æ ¹æ®æ‚¨çš„è®¾å®šï¼Œæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å†…å®¹ï¼Œè¯·ç¡®è®¤æ‚¨è®¾ç½®çš„çŸ¥è¯†ç›¸å…³åº¦ Score é˜ˆå€¼æ˜¯å¦è¿‡å°æˆ–å…¶ä»–å‚æ•°æ˜¯å¦æ­£ç¡®ã€‚"]], ""
            else:
                source = "\n".join(
                    [
                        f"""<details open> <summary>ã€çŸ¥è¯†ç›¸å…³åº¦ Scoreã€‘ï¼š{doc.metadata["score"]} - ã€å‡ºå¤„{i + 1}ã€‘ï¼š  {os.path.split(doc.metadata["source"])[-1]} </summary>\n"""
                        f"""{doc.page_content}\n"""
                        f"""</details>"""
                        for i, doc in
                        enumerate(resp["source_documents"])])
                history.append([query, "ä»¥ä¸‹å†…å®¹ä¸ºçŸ¥è¯†åº“ä¸­æ»¡è¶³è®¾ç½®æ¡ä»¶çš„åŒ¹é…ç»“æœï¼š\n\n" + source])
                yield history, ""
        else:
            yield history + [[query,
                              "è¯·é€‰æ‹©çŸ¥è¯†åº“åè¿›è¡Œæµ‹è¯•ï¼Œå½“å‰æœªé€‰æ‹©çŸ¥è¯†åº“ã€‚"]], ""
    else:
        for answer_result in local_doc_qa.llm.generatorAnswer(prompt=query, history=history,
                                                              streaming=streaming):

            resp = answer_result.llm_output["answer"]
            history = answer_result.history
            history[-1][-1] = resp + (
                "\n\nå½“å‰çŸ¥è¯†åº“ä¸ºç©ºï¼Œå¦‚éœ€åŸºäºçŸ¥è¯†åº“è¿›è¡Œé—®ç­”ï¼Œè¯·å…ˆåŠ è½½çŸ¥è¯†åº“åï¼Œå†è¿›è¡Œæé—®ã€‚" if mode == "çŸ¥è¯†åº“é—®ç­”" else "")
            yield history, ""
    logger.info(f"flagging: username={FLAG_USER_NAME},query={query},vs_path={vs_path},mode={mode},history={history}")
    # flag_csv_logger.flag([query, vs_path, history, mode], username=FLAG_USER_NAME)


def init_model(llm_model: str = 'chat-glm-6b', embedding_model: str = 'text2vec'):
    local_doc_qa = LocalDocQA()
    # åˆå§‹åŒ–æ¶ˆæ¯
    args = parser.parse_args()
    args_dict = vars(args)
    args_dict.update(model=llm_model)
    shared.loaderCheckPoint = LoaderCheckPoint(args_dict)
    llm_model_ins = shared.loaderLLM()
    llm_model_ins.set_history_len(LLM_HISTORY_LEN)

    try:
        local_doc_qa.init_cfg(llm_model=llm_model_ins,
                              embedding_model=embedding_model)
        generator = local_doc_qa.llm.generatorAnswer("ä½ å¥½")
        for answer_result in generator:
            print(answer_result.llm_output)
        reply = """æ¨¡å‹å·²æˆåŠŸåŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¯¹è¯ï¼Œæˆ–ä»å³ä¾§é€‰æ‹©æ¨¡å¼åå¼€å§‹å¯¹è¯"""
        logger.info(reply)
    except Exception as e:
        logger.error(e)
        reply = """æ¨¡å‹æœªæˆåŠŸåŠ è½½ï¼Œè¯·åˆ°é¡µé¢å·¦ä¸Šè§’"æ¨¡å‹é…ç½®"é€‰é¡¹å¡ä¸­é‡æ–°é€‰æ‹©åç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®"""
        if str(e) == "Unknown platform: darwin":
            logger.info("è¯¥æŠ¥é”™å¯èƒ½å› ä¸ºæ‚¨ä½¿ç”¨çš„æ˜¯ macOS æ“ä½œç³»ç»Ÿï¼Œéœ€å…ˆä¸‹è½½æ¨¡å‹è‡³æœ¬åœ°åæ‰§è¡Œ Web UIï¼Œå…·ä½“æ–¹æ³•è¯·å‚è€ƒé¡¹ç›® README ä¸­æœ¬åœ°éƒ¨ç½²æ–¹æ³•åŠå¸¸è§é—®é¢˜ï¼š"
                        " https://github.com/imClumsyPanda/langchain-ChatGLM")
        else:
            logger.info(reply)
    return local_doc_qa


# æš‚æœªä½¿ç”¨åˆ°ï¼Œå…ˆä¿ç•™
# def reinit_model(llm_model, embedding_model, llm_history_len, no_remote_model, use_ptuning_v2, use_lora, top_k, history):
#     try:
#         llm_model_ins = shared.loaderLLM(llm_model, no_remote_model, use_ptuning_v2)
#         llm_model_ins.history_len = llm_history_len
#         local_doc_qa.init_cfg(llm_model=llm_model_ins,
#                               embedding_model=embedding_model,
#                               top_k=top_k)
#         model_status = """æ¨¡å‹å·²æˆåŠŸé‡æ–°åŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¯¹è¯ï¼Œæˆ–ä»å³ä¾§é€‰æ‹©æ¨¡å¼åå¼€å§‹å¯¹è¯"""
#         logger.info(model_status)
#     except Exception as e:
#         logger.error(e)
#         model_status = """æ¨¡å‹æœªæˆåŠŸé‡æ–°åŠ è½½ï¼Œè¯·åˆ°é¡µé¢å·¦ä¸Šè§’"æ¨¡å‹é…ç½®"é€‰é¡¹å¡ä¸­é‡æ–°é€‰æ‹©åç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®"""
#         logger.info(model_status)
#     return history + [[None, model_status]]


def get_vector_store(vs_id, files, sentence_size, history, one_conent, one_content_segmentation):
    vs_path = os.path.join(KB_ROOT_PATH, vs_id, "vector_store")
    filelist = []
    if not os.path.exists(os.path.join(KB_ROOT_PATH, vs_id, "content")):
        os.makedirs(os.path.join(KB_ROOT_PATH, vs_id, "content"))
    if local_doc_qa.llm and local_doc_qa.embeddings:
        if isinstance(files, list):
            for file in files:
                filename = os.path.split(file.name)[-1]
                shutil.move(file.name, os.path.join(
                    KB_ROOT_PATH, vs_id, "content", filename))
                filelist.append(os.path.join(
                    KB_ROOT_PATH, vs_id, "content", filename))
            vs_path, loaded_files = local_doc_qa.init_knowledge_vector_store(
                filelist, vs_path, sentence_size)
        else:
            vs_path, loaded_files = local_doc_qa.one_knowledge_add(vs_path, files, one_conent, one_content_segmentation,
                                                                   sentence_size)
        if len(loaded_files):
            file_status = f"å·²æ·»åŠ  {'ã€'.join([os.path.split(i)[-1] for i in loaded_files if i])} å†…å®¹è‡³çŸ¥è¯†åº“ï¼Œå¹¶å·²åŠ è½½çŸ¥è¯†åº“ï¼Œè¯·å¼€å§‹æé—®"
        else:
            file_status = "æ–‡ä»¶æœªæˆåŠŸåŠ è½½ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡ä»¶"
    else:
        file_status = "æ¨¡å‹æœªå®ŒæˆåŠ è½½ï¼Œè¯·å…ˆåœ¨åŠ è½½æ¨¡å‹åå†å¯¼å…¥æ–‡ä»¶"
        vs_path = None
    logger.info(file_status)
    return vs_path, None, history + [[None, file_status]]


knowledge_base_test_mode_info = ("ã€æ³¨æ„ã€‘\n\n"
                                 "1. æ‚¨å·²è¿›å…¥çŸ¥è¯†åº“æµ‹è¯•æ¨¡å¼ï¼Œæ‚¨è¾“å…¥çš„ä»»ä½•å¯¹è¯å†…å®¹éƒ½å°†ç”¨äºè¿›è¡ŒçŸ¥è¯†åº“æŸ¥è¯¢ï¼Œ"
                                 "å¹¶ä»…è¾“å‡ºçŸ¥è¯†åº“åŒ¹é…å‡ºçš„å†…å®¹åŠç›¸ä¼¼åº¦åˆ†å€¼å’ŒåŠè¾“å…¥çš„æ–‡æœ¬æºè·¯å¾„ï¼ŒæŸ¥è¯¢çš„å†…å®¹å¹¶ä¸ä¼šè¿›å…¥æ¨¡å‹æŸ¥è¯¢ã€‚\n\n"
                                 "2. çŸ¥è¯†ç›¸å…³åº¦ Score ç»æµ‹è¯•ï¼Œå»ºè®®è®¾ç½®ä¸º 500 æˆ–æ›´ä½ï¼Œå…·ä½“è®¾ç½®æƒ…å†µè¯·ç»“åˆå®é™…ä½¿ç”¨è°ƒæ•´ã€‚"
                                 """3. ä½¿ç”¨"æ·»åŠ å•æ¡æ•°æ®"æ·»åŠ æ–‡æœ¬è‡³çŸ¥è¯†åº“æ—¶ï¼Œå†…å®¹å¦‚æœªåˆ†æ®µï¼Œåˆ™å†…å®¹è¶Šå¤šè¶Šä¼šç¨€é‡Šå„æŸ¥è¯¢å†…å®¹ä¸ä¹‹å…³è”çš„scoreé˜ˆå€¼ã€‚\n\n"""
                                 "4. å•æ¡å†…å®¹é•¿åº¦å»ºè®®è®¾ç½®åœ¨100-150å·¦å³ã€‚\n\n"
                                 "5. æœ¬ç•Œé¢ç”¨äºçŸ¥è¯†å…¥åº“åŠçŸ¥è¯†åŒ¹é…ç›¸å…³å‚æ•°è®¾å®šï¼Œä½†å½“å‰ç‰ˆæœ¬ä¸­ï¼Œ"
                                 "æœ¬ç•Œé¢ä¸­ä¿®æ”¹çš„å‚æ•°å¹¶ä¸ä¼šç›´æ¥ä¿®æ”¹å¯¹è¯ç•Œé¢ä¸­å‚æ•°ï¼Œä»éœ€å‰å¾€`configs/model_config.py`ä¿®æ”¹åç”Ÿæ•ˆã€‚"
                                 "ç›¸å…³å‚æ•°å°†åœ¨åç»­ç‰ˆæœ¬ä¸­æ”¯æŒæœ¬ç•Œé¢ç›´æ¥ä¿®æ”¹ã€‚")


webui_title = """
# ğŸ‰langchain-ChatGLM WebUIğŸ‰
ğŸ‘ [https://github.com/imClumsyPanda/langchain-ChatGLM](https://github.com/imClumsyPanda/langchain-ChatGLM)
"""
######                   #####


###### todo #####
# 1. streamlitè¿è¡Œæ–¹å¼ä¸ä¸€èˆ¬webæœåŠ¡å™¨ä¸åŒï¼Œä½¿ç”¨æ¨¡å—æ˜¯æ— æ³•å®ç°å•ä¾‹æ¨¡å¼çš„ï¼Œæ‰€ä»¥sharedå’Œlocal_doc_qaéƒ½éœ€è¦è¿›è¡Œå…¨å±€åŒ–å¤„ç†ã€‚
#   ç›®å‰å·²ç»å®ç°äº†local_doc_qaçš„å…¨å±€åŒ–ï¼Œåé¢è¦è€ƒè™‘sharedã€‚
# 2. å½“å‰local_doc_qaæ˜¯ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œä¸€æ–¹é¢ï¼šä»»ä½•ä¸€ä¸ªsessionå¯¹å…¶åšå‡ºä¿®æ”¹ï¼Œéƒ½ä¼šå½±å“æ‰€æœ‰sessionçš„å¯¹è¯;å¦ä¸€æ–¹é¢ï¼Œå¦‚ä½•å¤„ç†æ‰€æœ‰sessionçš„è¯·æ±‚ç«äº‰ä¹Ÿæ˜¯é—®é¢˜ã€‚
#   è¿™ä¸ªæš‚æ—¶æ— æ³•é¿å…ï¼Œåœ¨é…ç½®æ™®é€šçš„æœºå™¨ä¸Šæš‚æ—¶ä¹Ÿæ— éœ€è€ƒè™‘ã€‚
# 3. ç›®å‰åªåŒ…å«äº†get_answerå¯¹åº”çš„å‚æ•°ï¼Œä»¥åå¯ä»¥æ·»åŠ å…¶ä»–å‚æ•°ï¼Œå¦‚temperatureã€‚
######      #####


###### é…ç½®é¡¹ #####
class ST_CONFIG:
    user_bg_color = '#77ff77'
    user_icon = 'https://tse2-mm.cn.bing.net/th/id/OIP-C.LTTKrxNWDr_k74wz6jKqBgHaHa?w=203&h=203&c=7&r=0&o=5&pid=1.7'
    robot_bg_color = '#ccccee'
    robot_icon = 'https://ts1.cn.mm.bing.net/th/id/R-C.5302e2cc6f5c7c4933ebb3394e0c41bc?rik=z4u%2b7efba5Mgxw&riu=http%3a%2f%2fcomic-cons.xyz%2fwp-content%2fuploads%2fStar-Wars-avatar-icon-C3PO.png&ehk=kBBvCvpJMHPVpdfpw1GaH%2brbOaIoHjY5Ua9PKcIs%2bAc%3d&risl=&pid=ImgRaw&r=0'
    default_mode = 'çŸ¥è¯†åº“é—®ç­”'
    defalut_kb = ''
######        #####


class MsgType:
    '''
    ç›®å‰ä»…æ”¯æŒæ–‡æœ¬ç±»å‹çš„è¾“å…¥è¾“å‡ºï¼Œä¸ºä»¥åå¤šæ¨¡æ€æ¨¡å‹é¢„ç•™å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘æ”¯æŒã€‚
    '''
    TEXT = 1
    IMAGE = 2
    VIDEO = 3
    AUDIO = 4


class TempFile:
    '''
    ä¸ºä¿æŒä¸get_vector_storeçš„å…¼å®¹æ€§ï¼Œéœ€è¦å°†streamlitä¸Šä¼ æ–‡ä»¶è½¬åŒ–ä¸ºå…¶å¯ä»¥æ¥å—çš„æ–¹å¼
    '''

    def __init__(self, path):
        self.name = path


def init_session():
    st.session_state.setdefault('history', [])


# def get_query_params():
#     '''
#     å¯ä»¥ç”¨urlå‚æ•°ä¼ é€’é…ç½®å‚æ•°ï¼šllm_model, embedding_model, kb, modeã€‚
#     è¯¥å‚æ•°å°†è¦†ç›–model_configä¸­çš„é…ç½®ã€‚å¤„äºå®‰å…¨è€ƒè™‘ï¼Œç›®å‰åªæ”¯æŒkbå’Œmode
#     æ–¹ä¾¿å°†å›ºå®šçš„é…ç½®åˆ†äº«ç»™ç‰¹å®šçš„äººã€‚
#     '''
#     params = st.experimental_get_query_params()
#     return {k: v[0] for k, v in params.items() if v}


def robot_say(msg, kb=''):
    st.session_state['history'].append(
        {'is_user': False, 'type': MsgType.TEXT, 'content': msg, 'kb': kb})


def user_say(msg):
    st.session_state['history'].append(
        {'is_user': True, 'type': MsgType.TEXT, 'content': msg})


def format_md(msg, is_user=False, bg_color='', margin='10%'):
    '''
    å°†æ–‡æœ¬æ¶ˆæ¯æ ¼å¼åŒ–ä¸ºmarkdownæ–‡æœ¬
    '''
    if is_user:
        bg_color = bg_color or ST_CONFIG.user_bg_color
        text = f'''
                <div style="background:{bg_color};
                        margin-left:{margin};
                        word-break:break-all;
                        float:right;
                        padding:2%;
                        border-radius:2%;">
                {msg}
                </div>
                '''
    else:
        bg_color = bg_color or ST_CONFIG.robot_bg_color
        text = f'''
                <div style="background:{bg_color};
                        margin-right:{margin};
                        word-break:break-all;
                        padding:2%;
                        border-radius:2%;">
                {msg}
                </div>
                '''
    return text


def message(msg,
            is_user=False,
            msg_type=MsgType.TEXT,
            icon='',
            bg_color='',
            margin='10%',
            kb='',
            ):
    '''
    æ¸²æŸ“å•æ¡æ¶ˆæ¯ã€‚ç›®å‰ä»…æ”¯æŒæ–‡æœ¬
    '''
    cols = st.columns([1, 10, 1])
    empty = cols[1].empty()
    if is_user:
        icon = icon or ST_CONFIG.user_icon
        bg_color = bg_color or ST_CONFIG.user_bg_color
        cols[2].image(icon, width=40)
        if msg_type == MsgType.TEXT:
            text = format_md(msg, is_user, bg_color, margin)
            empty.markdown(text, unsafe_allow_html=True)
        else:
            raise RuntimeError('only support text message now.')
    else:
        icon = icon or ST_CONFIG.robot_icon
        bg_color = bg_color or ST_CONFIG.robot_bg_color
        cols[0].image(icon, width=40)
        if kb:
            cols[0].write(f'({kb})')
        if msg_type == MsgType.TEXT:
            text = format_md(msg, is_user, bg_color, margin)
            empty.markdown(text, unsafe_allow_html=True)
        else:
            raise RuntimeError('only support text message now.')
    return empty


def output_messages(
    user_bg_color='',
    robot_bg_color='',
    user_icon='',
    robot_icon='',
):
    with chat_box.container():
        last_response = None
        for msg in st.session_state['history']:
            bg_color = user_bg_color if msg['is_user'] else robot_bg_color
            icon = user_icon if msg['is_user'] else robot_icon
            empty = message(msg['content'],
                            is_user=msg['is_user'],
                            icon=icon,
                            msg_type=msg['type'],
                            bg_color=bg_color,
                            kb=msg.get('kb', '')
                            )
            if not msg['is_user']:
                last_response = empty
    return last_response


@st.cache_resource(show_spinner=False, max_entries=1)
def load_model(llm_model: str, embedding_model: str):
    '''
    å¯¹åº”init_modelï¼Œåˆ©ç”¨streamlit cacheé¿å…æ¨¡å‹é‡å¤åŠ è½½
    '''
    local_doc_qa = init_model(llm_model, embedding_model)
    robot_say('æ¨¡å‹å·²æˆåŠŸåŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¯¹è¯ï¼Œæˆ–ä»å·¦ä¾§é€‰æ‹©æ¨¡å¼åå¼€å§‹å¯¹è¯ã€‚\nè¯·å°½é‡ä¸è¦åˆ·æ–°é¡µé¢ï¼Œä»¥å…æ¨¡å‹å‡ºé”™æˆ–é‡å¤åŠ è½½ã€‚')
    return local_doc_qa


# @st.cache_data
def answer(query, vs_path='', history=[], mode='', score_threshold=0,
           vector_search_top_k=5, chunk_conent=True, chunk_size=100, qa=None
           ):
    '''
    å¯¹åº”get_answerï¼Œ--åˆ©ç”¨streamlit cacheç¼“å­˜ç›¸åŒé—®é¢˜çš„ç­”æ¡ˆ--
    '''
    return get_answer(query, vs_path, history, mode, score_threshold,
                      vector_search_top_k, chunk_conent, chunk_size)


def load_vector_store(
    vs_id,
    files,
    sentence_size=100,
    history=[],
    one_conent=None,
    one_content_segmentation=None,
):
    return get_vector_store(
        local_doc_qa,
        vs_id,
        files,
        sentence_size,
        history,
        one_conent,
        one_content_segmentation,
    )


# main ui
st.set_page_config(webui_title, layout='wide')
init_session()
# params = get_query_params()
# llm_model = params.get('llm_model', LLM_MODEL)
# embedding_model = params.get('embedding_model', EMBEDDING_MODEL)

with st.spinner(f'æ­£åœ¨åŠ è½½æ¨¡å‹({LLM_MODEL} + {EMBEDDING_MODEL})ï¼Œè¯·è€å¿ƒç­‰å€™...'):
    local_doc_qa = load_model(LLM_MODEL, EMBEDDING_MODEL)


def use_kb_mode(m):
    return m in ['çŸ¥è¯†åº“é—®ç­”', 'çŸ¥è¯†åº“æµ‹è¯•']


# sidebar
modes = ['LLM å¯¹è¯', 'çŸ¥è¯†åº“é—®ç­”', 'Bingæœç´¢é—®ç­”', 'çŸ¥è¯†åº“æµ‹è¯•']
with st.sidebar:
    def on_mode_change():
        m = st.session_state.mode
        robot_say(f'å·²åˆ‡æ¢åˆ°"{m}"æ¨¡å¼')
        if m == 'çŸ¥è¯†åº“æµ‹è¯•':
            robot_say(knowledge_base_test_mode_info)

    index = 0
    try:
        index = modes.index(ST_CONFIG.default_mode)
    except:
        pass
    mode = st.selectbox('å¯¹è¯æ¨¡å¼', modes, index,
                        on_change=on_mode_change, key='mode')

    with st.expander('æ¨¡å‹é…ç½®', 'çŸ¥è¯†' not in mode):
        with st.form('model_config'):
            index = 0
            try:
                index = llm_model_dict_list.index(LLM_MODEL)
            except:
                pass
            llm_model = st.selectbox('LLMæ¨¡å‹', llm_model_dict_list, index)

            no_remote_model = st.checkbox('åŠ è½½æœ¬åœ°æ¨¡å‹', False)
            use_ptuning_v2 = st.checkbox('ä½¿ç”¨p-tuning-v2å¾®è°ƒè¿‡çš„æ¨¡å‹', False)
            use_lora = st.checkbox('ä½¿ç”¨loraå¾®è°ƒçš„æƒé‡', False)
            try:
                index = embedding_model_dict_list.index(EMBEDDING_MODEL)
            except:
                pass
            embedding_model = st.selectbox(
                'Embeddingæ¨¡å‹', embedding_model_dict_list, index)

            btn_load_model = st.form_submit_button('é‡æ–°åŠ è½½æ¨¡å‹')
            if btn_load_model:
                local_doc_qa = load_model(llm_model, embedding_model)

    if mode in ['çŸ¥è¯†åº“é—®ç­”', 'çŸ¥è¯†åº“æµ‹è¯•']:
        vs_list = get_vs_list()
        vs_list.remove('æ–°å»ºçŸ¥è¯†åº“')

        def on_new_kb():
            name = st.session_state.kb_name
            if name in vs_list:
                st.error(f'åä¸ºâ€œ{name}â€çš„çŸ¥è¯†åº“å·²å­˜åœ¨ã€‚')
            else:
                vs_list.append(name)
                st.session_state.vs_path = name

        def on_vs_change():
            robot_say(f'å·²åŠ è½½çŸ¥è¯†åº“ï¼š {st.session_state.vs_path}')
        with st.expander('çŸ¥è¯†åº“é…ç½®', True):
            cols = st.columns([12, 10])
            kb_name = cols[0].text_input(
                'æ–°çŸ¥è¯†åº“åç§°', placeholder='æ–°çŸ¥è¯†åº“åç§°', label_visibility='collapsed')
            cols[1].button('æ–°å»ºçŸ¥è¯†åº“', on_click=on_new_kb)
            vs_path = st.selectbox(
                'é€‰æ‹©çŸ¥è¯†åº“', vs_list, on_change=on_vs_change, key='vs_path')

            st.text('')

            score_threshold = st.slider(
                'çŸ¥è¯†ç›¸å…³åº¦é˜ˆå€¼', 0, 1000, VECTOR_SEARCH_SCORE_THRESHOLD)
            top_k = st.slider('å‘é‡åŒ¹é…æ•°é‡', 1, 20, VECTOR_SEARCH_TOP_K)
            history_len = st.slider(
                'LLMå¯¹è¯è½®æ•°', 1, 50, LLM_HISTORY_LEN)  # ä¹Ÿè®¸è¦è·ŸçŸ¥è¯†åº“åˆ†å¼€è®¾ç½®
            local_doc_qa.llm.set_history_len(history_len)
            chunk_conent = st.checkbox('å¯ç”¨ä¸Šä¸‹æ–‡å…³è”', False)
            st.text('')
            # chunk_conent = st.checkbox('åˆ†å‰²æ–‡æœ¬', True) # çŸ¥è¯†åº“æ–‡æœ¬åˆ†å‰²å…¥åº“
            chunk_size = st.slider('ä¸Šä¸‹æ–‡å…³è”é•¿åº¦', 1, 1000, CHUNK_SIZE)
            sentence_size = st.slider('æ–‡æœ¬å…¥åº“åˆ†å¥é•¿åº¦é™åˆ¶', 1, 1000, SENTENCE_SIZE)
            files = st.file_uploader('ä¸Šä¼ çŸ¥è¯†æ–‡ä»¶',
                                     ['docx', 'txt', 'md', 'csv', 'xlsx', 'pdf'],
                                     accept_multiple_files=True)
            if st.button('æ·»åŠ æ–‡ä»¶åˆ°çŸ¥è¯†åº“'):
                temp_dir = tempfile.mkdtemp()
                file_list = []
                for f in files:
                    file = os.path.join(temp_dir, f.name)
                    with open(file, 'wb') as fp:
                        fp.write(f.getvalue())
                    file_list.append(TempFile(file))
                _, _, history = load_vector_store(
                    vs_path, file_list, sentence_size, [], None, None)
                st.session_state.files = []


# main body
chat_box = st.empty()

with st.form('my_form', clear_on_submit=True):
    cols = st.columns([8, 1])
    question = cols[0].text_input(
        'temp', key='input_question', label_visibility='collapsed')

    def on_send():
        q = st.session_state.input_question
        if q:
            user_say(q)

            if mode == 'LLM å¯¹è¯':
                robot_say('æ­£åœ¨æ€è€ƒ...')
                last_response = output_messages()
                for history, _ in answer(q,
                                         history=[],
                                         mode=mode):
                    last_response.markdown(
                        format_md(history[-1][-1], False),
                        unsafe_allow_html=True
                    )
            elif use_kb_mode(mode):
                robot_say('æ­£åœ¨æ€è€ƒ...', vs_path)
                last_response = output_messages()
                for history, _ in answer(q,
                                         vs_path=os.path.join(
                                             KB_ROOT_PATH, vs_path, "vector_store"),
                                         history=[],
                                         mode=mode,
                                         score_threshold=score_threshold,
                                         vector_search_top_k=top_k,
                                         chunk_conent=chunk_conent,
                                         chunk_size=chunk_size):
                    last_response.markdown(
                        format_md(history[-1][-1], False, 'ligreen'),
                        unsafe_allow_html=True
                    )
            else:
                robot_say('æ­£åœ¨æ€è€ƒ...')
                last_response = output_messages()
            st.session_state['history'][-1]['content'] = history[-1][-1]
    submit = cols[1].form_submit_button('å‘é€', on_click=on_send)

output_messages()

# st.write(st.session_state['history'])
